{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf0EKMiEJqvJl7rWFK7zXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naman09746/ML-practise/blob/main/AI_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter-1**"
      ],
      "metadata": {
        "id": "CE1728v5Pb-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t2Anm2GhatQ",
        "outputId": "5e59a82a-2252-489e-bb1d-5940366b8658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 8.9544\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2489\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.9029\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8398\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9994\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3343\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8071\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3886\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0556\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.7901\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5776\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4069\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2692\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1576\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0665\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9916\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9296\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8777\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8339\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7964\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7641\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7358\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7108\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6885\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6682\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6497\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6325\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6166\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6016\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5874\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5738\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5609\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5485\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5365\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5250\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5137\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5028\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4922\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4819\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4718\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4620\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4524\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4431\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4339\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4249\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4162\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4076\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3992\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3910\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3829\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3750\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3673\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3598\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3524\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3451\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3380\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3311\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3243\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3176\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3111\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3047\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2985\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2923\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2863\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2804\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2747\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2690\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2635\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2581\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2528\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2476\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2425\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2375\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2327\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2279\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2232\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2186\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2141\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2097\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2054\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2012\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1971\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1930\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1890\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1852\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1814\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1776\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1740\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1704\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1669\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1635\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1601\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1568\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1536\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1505\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1474\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1443\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1414\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1385\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1356\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1328\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1301\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1274\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1248\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1223\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1197\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1173\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1149\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1125\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1102\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1079\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1057\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1036\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1014\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0993\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0973\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0953\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0933\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0914\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0896\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0877\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0859\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0841\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0824\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0807\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0791\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0774\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0759\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0743\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0728\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0713\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0698\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0684\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0670\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0656\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0642\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0629\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0616\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0604\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0591\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0579\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0567\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0556\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0544\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0533\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0522\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0511\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0501\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0491\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0480\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0471\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0461\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0451\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0442\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0433\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0424\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0416\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0407\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0399\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0390\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0382\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0375\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0367\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0359\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0352\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0345\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0338\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0331\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0324\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0317\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0311\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0304\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0298\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0292\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0286\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0280\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0274\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0269\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0263\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0258\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0252\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0247\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0242\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0237\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0232\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0228\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0223\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0218\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0214\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0209\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0205\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0201\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0197\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0193\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0189\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0185\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0181\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0177\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0174\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0170\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0167\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0163\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0160\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0157\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0153\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0150\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0147\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0144\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0141\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0138\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0135\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0133\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0130\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0127\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0125\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0122\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0120\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0117\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0115\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0112\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0110\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0108\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0106\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0103\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0101\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0099\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0097\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0095\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0093\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0091\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0089\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0088\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0086\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0084\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0082\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0081\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0079\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0077\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0076\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0074\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0073\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0071\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0070\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0068\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0067\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0066\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0064\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0063\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0062\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0060\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0059\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0058\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0057\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0055\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0054\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0053\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0052\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0051\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0050\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0049\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0048\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0047\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0046\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0045\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0044\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0043\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0042\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0042\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0041\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0040\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0039\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0038\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0037\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0037\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0036\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0035\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0034\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0034\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0033\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0032\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0032\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0031\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0030\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0030\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0029\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0029\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0028\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0027\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0027\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0026\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0025\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0025\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0024\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0024\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0023\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0023\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0022\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0022\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0021\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0021\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0020\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0020\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0019\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0018\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0018\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0018\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0017\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0017\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0017\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0015\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0014\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0014\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0014\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0014\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0013\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0013\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0013\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0012\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0012\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0012\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0012\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0011\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0011\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0010\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.9114e-04\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.7078e-04\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.5084e-04\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.3131e-04\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.1218e-04\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.9344e-04\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.7509e-04\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.5712e-04\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.3951e-04\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.2227e-04\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.0538e-04\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.8884e-04\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.7263e-04\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.5676e-04\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.4122e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2599e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1108e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9648e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.8217e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.6816e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.5443e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.4099e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.2783e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.1493e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.0230e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.8993e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.7781e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.6594e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.5431e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.4293e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.3178e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.2086e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.1015e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.9968e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8941e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7936e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6951e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.5987e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.5043e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.4117e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.3211e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.2324e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.1454e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.0603e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.9769e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.8952e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.8152e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.7368e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.6601e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.5849e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.5113e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.4392e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.3685e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.2993e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.2316e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.1652e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.1002e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0365e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9741e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9130e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8532e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.7946e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7372e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6810e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6259e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5719e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5191e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.4674e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4167e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3670e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3184e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2708e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2242e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1785e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1337e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0899e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0470e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0049e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9637e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9234e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8839e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8452e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8073e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7702e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7338e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6982e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.6633e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6292e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5957e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5629e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5308e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4994e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4686e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4384e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4089e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3799e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3516e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3238e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2966e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2700e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2439e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2183e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1933e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1688e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1448e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1213e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0983e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0757e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0536e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0320e-04\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0108e-04\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.9002e-05\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.6968e-05\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.4976e-05\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.3024e-05\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 9.1115e-05\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.9243e-05\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.7409e-05\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.5614e-05\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.3857e-05\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.2134e-05\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.0448e-05\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.8794e-05\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.7176e-05\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.5590e-05\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.4038e-05\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.2516e-05\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.1026e-05\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.9567e-05\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8138e-05\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.6738e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.5367e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.4025e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.2710e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.1423e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.0161e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.8925e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.7714e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.6529e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.5369e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.4232e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.3117e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.2026e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.0957e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.9910e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.8885e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7880e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.6897e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.5934e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.4991e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4066e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.3161e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.2275e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.1407e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.0556e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9722e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8906e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8108e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7325e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6559e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.5808e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.5072e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.4352e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3646e-05\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "[[18.983078]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model = Sequential([Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "model.fit(xs, ys, epochs=500)\n",
        "print(model.predict([10.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter-2**"
      ],
      "metadata": {
        "id": "drLpHwsJPi0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        ])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "metadata": {
        "id": "omXGZE18h2he",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de83f75f-db41-42a7-abdf-5a5c7ebd007e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.5008 - accuracy: 0.8231\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3795 - accuracy: 0.8633\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3408 - accuracy: 0.8753\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3148 - accuracy: 0.8853\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2959 - accuracy: 0.8904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d5f13b93340>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.evaluate(test_images, test_labels))\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "id": "2BBCPIhpMGjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c758e70-6448-473e-efc9-73eb38e5e67b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3445 - accuracy: 0.8746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34448641538619995, 0.8745999932289124]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopping Training\n",
        "import tensorflow as tf\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images/255.0\n",
        "# Extract the NumPy array from the tuple\n",
        "test_images = test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50,\n",
        "               callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7yoTpkjN1l_",
        "outputId": "8bf882fa-64dc-4e63-8a65-94974c346faa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4998 - accuracy: 0.8250\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3720 - accuracy: 0.8650\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3367 - accuracy: 0.8778\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3127 - accuracy: 0.8856\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2957 - accuracy: 0.8911\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2779 - accuracy: 0.8968\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2653 - accuracy: 0.9020\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2555 - accuracy: 0.9054\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2457 - accuracy: 0.9086\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2361 - accuracy: 0.9124\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2303 - accuracy: 0.9135\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2215 - accuracy: 0.9163\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2152 - accuracy: 0.9190\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2081 - accuracy: 0.9227\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2026 - accuracy: 0.9246\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1951 - accuracy: 0.9267\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1906 - accuracy: 0.9280\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1854 - accuracy: 0.9310\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1799 - accuracy: 0.9324\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1788 - accuracy: 0.9324\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1719 - accuracy: 0.9354\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1669 - accuracy: 0.9368\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1628 - accuracy: 0.9392\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1595 - accuracy: 0.9392\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1569 - accuracy: 0.9413\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1533 - accuracy: 0.9419\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1490 - accuracy: 0.9436\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1458 - accuracy: 0.9447\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1430 - accuracy: 0.9462\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1390 - accuracy: 0.9477\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1369 - accuracy: 0.9487\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1337 - accuracy: 0.9492\n",
            "Epoch 33/50\n",
            "1861/1875 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9518\n",
            "Reached 95% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1294 - accuracy: 0.9516\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d5f13b93520>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter-3**"
      ],
      "metadata": {
        "id": "4_4aWQq9SSnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "training_images = training_images.reshape(60000, 28, 28, 1)\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images = test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Conv2D(64, (3, 3), activation='relu',\n",
        "                      input_shape=(28, 28, 1)),\n",
        "          tf.keras.layers.MaxPooling2D(2, 2),\n",
        "          tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "          tf.keras.layers.MaxPooling2D(2,2),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "          tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "           loss='sparse_categorical_crossentropy',\n",
        "           metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50)\n",
        "model.evaluate(test_images, test_labels)\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BVHOBeJHSReJ",
        "outputId": "ea7e84a6-28c5-42fc-ad31-6bb67e0f29fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 95s 49ms/step - loss: 0.4371 - accuracy: 0.8417\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.2939 - accuracy: 0.8928\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.2450 - accuracy: 0.9092\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.2147 - accuracy: 0.9197\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 89s 47ms/step - loss: 0.1886 - accuracy: 0.9290\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.1646 - accuracy: 0.9377\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.1438 - accuracy: 0.9467\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 91s 49ms/step - loss: 0.1257 - accuracy: 0.9528\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.1116 - accuracy: 0.9575\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.0979 - accuracy: 0.9629\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 95s 51ms/step - loss: 0.0858 - accuracy: 0.9669\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0759 - accuracy: 0.9712\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 87s 47ms/step - loss: 0.0699 - accuracy: 0.9739\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0601 - accuracy: 0.9775\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0556 - accuracy: 0.9785\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0500 - accuracy: 0.9809\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0451 - accuracy: 0.9837\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0433 - accuracy: 0.9842\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.0402 - accuracy: 0.9849\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 85s 46ms/step - loss: 0.0366 - accuracy: 0.9863\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0338 - accuracy: 0.9879\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0324 - accuracy: 0.9880\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0301 - accuracy: 0.9894\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0288 - accuracy: 0.9894\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0272 - accuracy: 0.9904\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.0268 - accuracy: 0.9898\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 102s 54ms/step - loss: 0.0256 - accuracy: 0.9909\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 105s 56ms/step - loss: 0.0266 - accuracy: 0.9906\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 85s 46ms/step - loss: 0.0230 - accuracy: 0.9918\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0238 - accuracy: 0.9918\n",
            "Epoch 31/50\n",
            "  92/1875 [>.............................] - ETA: 1:09 - loss: 0.0249 - accuracy: 0.9918"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cfa62a3dfb16>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m            \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m            metrics=['accuracy'])\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mclassifications\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Making a model differentiating between human and horses\n",
        "# import urllib.request\n",
        "# import zipfile\n",
        "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/ horse-or-human.zip\"\n",
        "# file_name = \"horse-or-human.zip\"\n",
        "# training_dir = 'horse-or-human/training/'\n",
        "# urllib.request.urlretrieve(url, file_name)\n",
        "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "# zip_ref.extractall(training_dir)\n",
        "# zip_ref.close()\n"
      ],
      "metadata": {
        "id": "fUEg3JM0mJI4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.keras.models.Sequential([\n",
        "#       tf.keras.layers.Conv2D(16, (3,3), activation='relu' ,\n",
        "#                   input_shape=(300, 300, 3)),\n",
        "#       tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#       tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "#       tf.keras.layers.MaxPooling2D(2,2),\n",
        "#       tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#       tf.keras.layers.MaxPooling2D(2,2),\n",
        "#       tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#       tf.keras.layers.MaxPooling2D(2,2),\n",
        "#       tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#       tf.keras.layers.MaxPooling2D(2,2),\n",
        "#       tf.keras.layers.Flatten(),\n",
        "#       tf.keras.layers.Dense(512, activation='relu'),\n",
        "#       tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#     ])\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#            optimizer=RMSprop(lr=0.001),\n",
        "# metrics=['accuracy'])\n",
        "# #We train by using fit_generator and passing it the training_generator we created earlier:\n",
        "# history = model.fit_generator(\n",
        "#     train_generator,\n",
        "#     epochs=15\n",
        "# )\n"
      ],
      "metadata": {
        "id": "TnsFGBcXnQxT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation**"
      ],
      "metadata": {
        "id": "yk7tELgzpvQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VALIDATION PROCESS\n",
        "\n",
        "# validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com /validation-horse-or-human.zip\"\n",
        "# validation_file_name = \"validation-horse-or-human.zip\"\n",
        "# validation_dir = 'horse-or-human/validation/'\n",
        "# urllib.request.urlretrieve(validation_url, validation_file_name)\n",
        "# zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
        "# zip_ref.extractall(validation_dir)\n",
        "# zip_ref.close()\n",
        "# validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "# validation_generator = train_datagen.flow_from_directory(\n",
        "# validation_dir,\n",
        "# target_size=(300, 300),\n",
        "# class_mode='binary'\n",
        "# )\n",
        "# history = model.fit_generator(\n",
        "#       train_generator,\n",
        "#       epochs=15,\n",
        "#     )"
      ],
      "metadata": {
        "id": "vIo6ANQ9oBp_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image augmentation**"
      ],
      "metadata": {
        "id": "ZHZ6HKr3pmUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "G1L3MjRTpsS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning**"
      ],
      "metadata": {
        "id": "hGfQWLZVqTYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "weights_file = \"inception_v3.h5\"\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "          include_top=False,\n",
        "          weights=None)\n",
        "pre_trained_model.load_weights(weights_file)\n",
        "\n",
        "pre_trained_model.summary()\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "metadata": {
        "id": "g4US35zEp0vC",
        "outputId": "5474ad3b-f58b-45a5-9241-c0657e37c5d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 74, 74, 32)           864       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 74, 74, 32)           96        ['conv2d_7[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 74, 74, 32)           0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 72, 72, 32)           9216      ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 72, 72, 32)           96        ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 72, 72, 32)           0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 72, 72, 64)           18432     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 72, 72, 64)           192       ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 72, 72, 64)           0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 35, 35, 64)           0         ['activation_2[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 35, 35, 80)           5120      ['max_pooling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 35, 35, 80)           240       ['conv2d_10[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 35, 35, 80)           0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 33, 33, 192)          138240    ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 33, 33, 192)          576       ['conv2d_11[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 33, 33, 192)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 16, 16, 192)          0         ['activation_4[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 16, 16, 64)           12288     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 16, 16, 64)           192       ['conv2d_15[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 16, 16, 64)           0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 48)           9216      ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 96)           55296     ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 16, 16, 48)           144       ['conv2d_13[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 16, 16, 96)           288       ['conv2d_16[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 16, 16, 48)           0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 16, 16, 96)           0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " average_pooling2d (Average  (None, 16, 16, 192)          0         ['max_pooling2d_8[0][0]']     \n",
            " Pooling2D)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 64)           12288     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 16, 16, 64)           76800     ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 16, 16, 96)           82944     ['activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 16, 16, 32)           6144      ['average_pooling2d[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 16, 16, 64)           192       ['conv2d_12[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 16, 16, 64)           192       ['conv2d_14[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 16, 16, 96)           288       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 16, 16, 32)           96        ['conv2d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 16, 16, 64)           0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 16, 16, 64)           0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 16, 16, 96)           0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 16, 16, 32)           0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)        (None, 16, 16, 256)          0         ['activation_5[0][0]',        \n",
            "                                                                     'activation_7[0][0]',        \n",
            "                                                                     'activation_10[0][0]',       \n",
            "                                                                     'activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 16, 16, 64)           16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 16, 16, 64)           192       ['conv2d_22[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 16, 16, 48)           12288     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 16, 16, 96)           55296     ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 16, 16, 48)           144       ['conv2d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 16, 16, 96)           288       ['conv2d_23[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 16, 16, 48)           0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 16, 16, 96)           0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (Avera  (None, 16, 16, 256)          0         ['mixed0[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 16, 16, 64)           16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 16, 16, 64)           76800     ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 16, 16, 96)           82944     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 64)           16384     ['average_pooling2d_1[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 16, 16, 64)           192       ['conv2d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 16, 16, 64)           192       ['conv2d_21[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 16, 16, 96)           288       ['conv2d_24[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 16, 16, 64)           192       ['conv2d_25[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 16, 16, 96)           0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)        (None, 16, 16, 288)          0         ['activation_12[0][0]',       \n",
            "                                                                     'activation_14[0][0]',       \n",
            "                                                                     'activation_17[0][0]',       \n",
            "                                                                     'activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 16, 16, 64)           18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 16, 16, 64)           192       ['conv2d_29[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 16, 16, 48)           13824     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)          (None, 16, 16, 96)           55296     ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 16, 16, 48)           144       ['conv2d_27[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 16, 16, 96)           288       ['conv2d_30[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 16, 16, 48)           0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 16, 16, 96)           0         ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (Avera  (None, 16, 16, 288)          0         ['mixed1[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 16, 16, 64)           18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 64)           76800     ['activation_20[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)          (None, 16, 16, 96)           82944     ['activation_23[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)          (None, 16, 16, 64)           18432     ['average_pooling2d_2[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 16, 16, 64)           192       ['conv2d_26[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 16, 16, 64)           192       ['conv2d_28[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 16, 16, 96)           288       ['conv2d_31[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 16, 16, 64)           192       ['conv2d_32[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 16, 16, 96)           0         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_25 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)        (None, 16, 16, 288)          0         ['activation_19[0][0]',       \n",
            "                                                                     'activation_21[0][0]',       \n",
            "                                                                     'activation_24[0][0]',       \n",
            "                                                                     'activation_25[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)          (None, 16, 16, 64)           18432     ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 16, 16, 64)           192       ['conv2d_34[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)          (None, 16, 16, 96)           55296     ['activation_27[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 16, 16, 96)           288       ['conv2d_35[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 16, 16, 96)           0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)          (None, 7, 7, 384)            995328    ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)          (None, 7, 7, 96)             82944     ['activation_28[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 7, 7, 384)            1152      ['conv2d_33[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 7, 7, 96)             288       ['conv2d_36[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_26 (Activation)  (None, 7, 7, 384)            0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 7, 7, 96)             0         ['batch_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 7, 7, 288)            0         ['mixed2[0][0]']              \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)        (None, 7, 7, 768)            0         ['activation_26[0][0]',       \n",
            "                                                                     'activation_29[0][0]',       \n",
            "                                                                     'max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)          (None, 7, 7, 128)            98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (Ba  (None, 7, 7, 128)            384       ['conv2d_41[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_34 (Activation)  (None, 7, 7, 128)            0         ['batch_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)          (None, 7, 7, 128)            114688    ['activation_34[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_35 (Ba  (None, 7, 7, 128)            384       ['conv2d_42[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_35 (Activation)  (None, 7, 7, 128)            0         ['batch_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)          (None, 7, 7, 128)            98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)          (None, 7, 7, 128)            114688    ['activation_35[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_31 (Ba  (None, 7, 7, 128)            384       ['conv2d_38[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_36 (Ba  (None, 7, 7, 128)            384       ['conv2d_43[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_31 (Activation)  (None, 7, 7, 128)            0         ['batch_normalization_31[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_36 (Activation)  (None, 7, 7, 128)            0         ['batch_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)          (None, 7, 7, 128)            114688    ['activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)          (None, 7, 7, 128)            114688    ['activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_32 (Ba  (None, 7, 7, 128)            384       ['conv2d_39[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_37 (Ba  (None, 7, 7, 128)            384       ['conv2d_44[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_32 (Activation)  (None, 7, 7, 128)            0         ['batch_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_37 (Activation)  (None, 7, 7, 128)            0         ['batch_normalization_37[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (Avera  (None, 7, 7, 768)            0         ['mixed3[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)          (None, 7, 7, 192)            172032    ['activation_32[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)          (None, 7, 7, 192)            172032    ['activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)          (None, 7, 7, 192)            147456    ['average_pooling2d_3[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_30 (Ba  (None, 7, 7, 192)            576       ['conv2d_37[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_33 (Ba  (None, 7, 7, 192)            576       ['conv2d_40[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_38 (Ba  (None, 7, 7, 192)            576       ['conv2d_45[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_39 (Ba  (None, 7, 7, 192)            576       ['conv2d_46[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_30 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_33 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_38 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_38[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_39 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_39[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)        (None, 7, 7, 768)            0         ['activation_30[0][0]',       \n",
            "                                                                     'activation_33[0][0]',       \n",
            "                                                                     'activation_38[0][0]',       \n",
            "                                                                     'activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)          (None, 7, 7, 160)            122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (Ba  (None, 7, 7, 160)            480       ['conv2d_51[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_44 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_44[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_45 (Ba  (None, 7, 7, 160)            480       ['conv2d_52[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_45 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_45[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)          (None, 7, 7, 160)            122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_41 (Ba  (None, 7, 7, 160)            480       ['conv2d_48[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_46 (Ba  (None, 7, 7, 160)            480       ['conv2d_53[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_41 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_41[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_46 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_41[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_46[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_42 (Ba  (None, 7, 7, 160)            480       ['conv2d_49[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_47 (Ba  (None, 7, 7, 160)            480       ['conv2d_54[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_42 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_47 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_47[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (Avera  (None, 7, 7, 768)            0         ['mixed4[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)          (None, 7, 7, 192)            215040    ['activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)          (None, 7, 7, 192)            215040    ['activation_47[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)          (None, 7, 7, 192)            147456    ['average_pooling2d_4[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_40 (Ba  (None, 7, 7, 192)            576       ['conv2d_47[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_43 (Ba  (None, 7, 7, 192)            576       ['conv2d_50[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_48 (Ba  (None, 7, 7, 192)            576       ['conv2d_55[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (Ba  (None, 7, 7, 192)            576       ['conv2d_56[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_40 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_40[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_43 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_43[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_48 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_48[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_49 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_49[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)        (None, 7, 7, 768)            0         ['activation_40[0][0]',       \n",
            "                                                                     'activation_43[0][0]',       \n",
            "                                                                     'activation_48[0][0]',       \n",
            "                                                                     'activation_49[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)          (None, 7, 7, 160)            122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_54 (Ba  (None, 7, 7, 160)            480       ['conv2d_61[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_54 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_54[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_54[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_55 (Ba  (None, 7, 7, 160)            480       ['conv2d_62[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_55 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_55[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)          (None, 7, 7, 160)            122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_55[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_51 (Ba  (None, 7, 7, 160)            480       ['conv2d_58[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_56 (Ba  (None, 7, 7, 160)            480       ['conv2d_63[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_51 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_51[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_56 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_56[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_51[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)          (None, 7, 7, 160)            179200    ['activation_56[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_52 (Ba  (None, 7, 7, 160)            480       ['conv2d_59[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_57 (Ba  (None, 7, 7, 160)            480       ['conv2d_64[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_52 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_52[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_57 (Activation)  (None, 7, 7, 160)            0         ['batch_normalization_57[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (Avera  (None, 7, 7, 768)            0         ['mixed5[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)          (None, 7, 7, 192)            215040    ['activation_52[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)          (None, 7, 7, 192)            215040    ['activation_57[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)          (None, 7, 7, 192)            147456    ['average_pooling2d_5[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_50 (Ba  (None, 7, 7, 192)            576       ['conv2d_57[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_53 (Ba  (None, 7, 7, 192)            576       ['conv2d_60[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_58 (Ba  (None, 7, 7, 192)            576       ['conv2d_65[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_59 (Ba  (None, 7, 7, 192)            576       ['conv2d_66[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_50 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_50[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_53 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_53[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_58 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_58[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_59 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_59[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)        (None, 7, 7, 768)            0         ['activation_50[0][0]',       \n",
            "                                                                     'activation_53[0][0]',       \n",
            "                                                                     'activation_58[0][0]',       \n",
            "                                                                     'activation_59[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_64 (Ba  (None, 7, 7, 192)            576       ['conv2d_71[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_64 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_64[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_64[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_65 (Ba  (None, 7, 7, 192)            576       ['conv2d_72[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_65 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_65[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_65[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_61 (Ba  (None, 7, 7, 192)            576       ['conv2d_68[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_66 (Ba  (None, 7, 7, 192)            576       ['conv2d_73[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_61 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_61[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_66[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_61[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_62 (Ba  (None, 7, 7, 192)            576       ['conv2d_69[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_67 (Ba  (None, 7, 7, 192)            576       ['conv2d_74[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_62 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_62[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_67[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (Avera  (None, 7, 7, 768)            0         ['mixed6[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_62[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)          (None, 7, 7, 192)            147456    ['average_pooling2d_6[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_60 (Ba  (None, 7, 7, 192)            576       ['conv2d_67[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_63 (Ba  (None, 7, 7, 192)            576       ['conv2d_70[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_68 (Ba  (None, 7, 7, 192)            576       ['conv2d_75[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_69 (Ba  (None, 7, 7, 192)            576       ['conv2d_76[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_60 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_60[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_63 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_63[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_68[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_69[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)        (None, 7, 7, 768)            0         ['activation_60[0][0]',       \n",
            "                                                                     'activation_63[0][0]',       \n",
            "                                                                     'activation_68[0][0]',       \n",
            "                                                                     'activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_72 (Ba  (None, 7, 7, 192)            576       ['conv2d_79[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_73 (Ba  (None, 7, 7, 192)            576       ['conv2d_80[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_73[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)          (None, 7, 7, 192)            147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)          (None, 7, 7, 192)            258048    ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_70 (Ba  (None, 7, 7, 192)            576       ['conv2d_77[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_74 (Ba  (None, 7, 7, 192)            576       ['conv2d_81[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_70[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 7, 7, 192)            0         ['batch_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)          (None, 3, 3, 320)            552960    ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)          (None, 3, 3, 192)            331776    ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_71 (Ba  (None, 3, 3, 320)            960       ['conv2d_78[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_75 (Ba  (None, 3, 3, 192)            576       ['conv2d_82[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 3, 3, 320)            0         ['batch_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 3, 3, 192)            0         ['batch_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooli  (None, 3, 3, 768)            0         ['mixed7[0][0]']              \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)        (None, 3, 3, 1280)           0         ['activation_71[0][0]',       \n",
            "                                                                     'activation_75[0][0]',       \n",
            "                                                                     'max_pooling2d_10[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)          (None, 3, 3, 448)            573440    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_80 (Ba  (None, 3, 3, 448)            1344      ['conv2d_87[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_80 (Activation)  (None, 3, 3, 448)            0         ['batch_normalization_80[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)          (None, 3, 3, 384)            491520    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)          (None, 3, 3, 384)            1548288   ['activation_80[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_77 (Ba  (None, 3, 3, 384)            1152      ['conv2d_84[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_81 (Ba  (None, 3, 3, 384)            1152      ['conv2d_88[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_77 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_77[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_81 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_81[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (Avera  (None, 3, 3, 1280)           0         ['mixed8[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)          (None, 3, 3, 320)            409600    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_78 (Ba  (None, 3, 3, 384)            1152      ['conv2d_85[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_79 (Ba  (None, 3, 3, 384)            1152      ['conv2d_86[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_82 (Ba  (None, 3, 3, 384)            1152      ['conv2d_89[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_83 (Ba  (None, 3, 3, 384)            1152      ['conv2d_90[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)          (None, 3, 3, 192)            245760    ['average_pooling2d_7[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_76 (Ba  (None, 3, 3, 320)            960       ['conv2d_83[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_78 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_78[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_79 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_79[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_82 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_82[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_83 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_83[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_84 (Ba  (None, 3, 3, 192)            576       ['conv2d_91[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 3, 3, 320)            0         ['batch_normalization_76[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)      (None, 3, 3, 768)            0         ['activation_78[0][0]',       \n",
            "                                                                     'activation_79[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 3, 3, 768)            0         ['activation_82[0][0]',       \n",
            "                                                                     'activation_83[0][0]']       \n",
            "                                                                                                  \n",
            " activation_84 (Activation)  (None, 3, 3, 192)            0         ['batch_normalization_84[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)        (None, 3, 3, 2048)           0         ['activation_76[0][0]',       \n",
            "                                                                     'mixed9_0[0][0]',            \n",
            "                                                                     'concatenate[0][0]',         \n",
            "                                                                     'activation_84[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_96 (Conv2D)          (None, 3, 3, 448)            917504    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_89 (Ba  (None, 3, 3, 448)            1344      ['conv2d_96[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_89 (Activation)  (None, 3, 3, 448)            0         ['batch_normalization_89[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)          (None, 3, 3, 384)            786432    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_97 (Conv2D)          (None, 3, 3, 384)            1548288   ['activation_89[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_86 (Ba  (None, 3, 3, 384)            1152      ['conv2d_93[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_90 (Ba  (None, 3, 3, 384)            1152      ['conv2d_97[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_86 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_86[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_90 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_90[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_94 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_95 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_98 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)          (None, 3, 3, 384)            442368    ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (Avera  (None, 3, 3, 2048)           0         ['mixed9[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)          (None, 3, 3, 320)            655360    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_87 (Ba  (None, 3, 3, 384)            1152      ['conv2d_94[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_88 (Ba  (None, 3, 3, 384)            1152      ['conv2d_95[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_91 (Ba  (None, 3, 3, 384)            1152      ['conv2d_98[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_92 (Ba  (None, 3, 3, 384)            1152      ['conv2d_99[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)         (None, 3, 3, 192)            393216    ['average_pooling2d_8[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_85 (Ba  (None, 3, 3, 320)            960       ['conv2d_92[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_87 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_87[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_88 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_88[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_91 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_91[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_92 (Activation)  (None, 3, 3, 384)            0         ['batch_normalization_92[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_93 (Ba  (None, 3, 3, 192)            576       ['conv2d_100[0][0]']          \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_85 (Activation)  (None, 3, 3, 320)            0         ['batch_normalization_85[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)      (None, 3, 3, 768)            0         ['activation_87[0][0]',       \n",
            "                                                                     'activation_88[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 3, 3, 768)            0         ['activation_91[0][0]',       \n",
            " )                                                                   'activation_92[0][0]']       \n",
            "                                                                                                  \n",
            " activation_93 (Activation)  (None, 3, 3, 192)            0         ['batch_normalization_93[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)       (None, 3, 3, 2048)           0         ['activation_85[0][0]',       \n",
            "                                                                     'mixed9_1[0][0]',            \n",
            "                                                                     'concatenate_1[0][0]',       \n",
            "                                                                     'activation_93[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21802784 (83.17 MB)\n",
            "Trainable params: 21768352 (83.04 MB)\n",
            "Non-trainable params: 34432 (134.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "last layer output shape:  (None, 7, 7, 768)\n"
          ]
        }
      ]
    }
  ]
}